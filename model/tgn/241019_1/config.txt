model: TemporalHeterogeneousGNN(
  (convs): ModuleList(
    (0-1): 2 x HeteroConv(num_relations=3)
    (2): HeteroConv(num_relations=1)
  )
  (player_temporal_layer): GConvGRU(
    (conv_x_z): ChebConv(5, 5, K=2, normalization=sym)
    (conv_h_z): ChebConv(5, 5, K=2, normalization=sym)
    (conv_x_r): ChebConv(5, 5, K=2, normalization=sym)
    (conv_h_r): ChebConv(5, 5, K=2, normalization=sym)
    (conv_x_h): ChebConv(5, 5, K=2, normalization=sym)
    (conv_h_h): ChebConv(5, 5, K=2, normalization=sym)
  )
  (map_temporal_layer): GConvGRU(
    (conv_x_z): ChebConv(3, 3, K=2, normalization=sym)
    (conv_h_z): ChebConv(3, 3, K=2, normalization=sym)
    (conv_x_r): ChebConv(3, 3, K=2, normalization=sym)
    (conv_h_r): ChebConv(3, 3, K=2, normalization=sym)
    (conv_x_h): ChebConv(3, 3, K=2, normalization=sym)
    (conv_h_h): ChebConv(3, 3, K=2, normalization=sym)
  )
  (linear): Linear(675, 48, bias=True)
  (dense): Sequential(
    (0): Linear(in_features=48, out_features=24, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=24, out_features=8, bias=True)
    (4): LeakyReLU(negative_slope=0.01)
    (5): Linear(in_features=8, out_features=8, bias=True)
    (6): LeakyReLU(negative_slope=0.01)
    (7): Dropout(p=0.5, inplace=False)
    (8): Linear(in_features=8, out_features=1, bias=True)
  )
)
dense_layers: [{'dropout': 0, 'num_of_layers': 1, 'neuron_num': 24, 'input_neuron_num': 48, 'activation_function': LeakyReLU(negative_slope=0.01)}, {'dropout': 0.5}, {'dropout': 0, 'num_of_layers': 2, 'neuron_num': 8, 'input_neuron_num': 24, 'activation_function': LeakyReLU(negative_slope=0.01)}, {'dropout': 0.5}, {'dropout': 0, 'num_of_layers': 1, 'neuron_num': 1, 'input_neuron_num': 8, 'activation_function': None}]
player_dims: [30, 5]
map_dims: [15, 10, 3]
player_attention_heads: None
map_attention_heads: None
trainable_params: 50933
optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    weight_decay: 0.055
)
loss_function: BCEWithLogitsLoss()
batch_size: 100
