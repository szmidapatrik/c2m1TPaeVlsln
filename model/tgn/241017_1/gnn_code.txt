
class TemporalHeterogeneousGNN(torch.nn.Module):

    # --------------------------------------------------
    # Initialization
    # --------------------------------------------------

    def __init__(self, player_dims, map_dims, dense_layers, player_temporal_dim=None, map_temporal_dim=None,  player_attention_heads=None, map_attention_heads=None):

        super().__init__()

        if player_temporal_dim is None:
            player_temporal_dim = player_dims[-1]
        if map_temporal_dim is None:
            map_temporal_dim = map_dims[-1]

        if player_attention_heads is not None and len(player_dims) != len(player_attention_heads):
            raise ValueError('The length of player dimensions and player attention heads arrays must be the same.')
        if map_attention_heads is not None and len(map_dims) != len(map_attention_heads):
            raise ValueError('The length of map dimensions and map attention heads arrays must be the same.')

        self.map_convs = len(map_dims)
        self.player_convs = len(player_dims)
        self.conv_layer_number = max([self.map_convs, self.player_convs])

        
        # Create convolutional layers
        self.convs = torch.nn.ModuleList()
        for conv_idx in range(self.conv_layer_number):

            layer_config = {}

            if conv_idx < len(player_dims):
                if player_attention_heads is None:
                    layer_config[('player', 'is', 'player')] = GATv2Conv((-1, -1), player_dims[conv_idx], add_self_loops=False)
                else:
                    layer_config[('player', 'is', 'player')] = GATv2Conv((-1, -1), player_dims[conv_idx], add_self_loops=False, heads=player_attention_heads[conv_idx])

            if conv_idx < len(player_dims):
                # GAT
                layer_config[('player', 'closest_to', 'map')] = GATv2Conv((-1, -1), map_dims[conv_idx], add_self_loops=False)


                
            if conv_idx < len(map_dims):

                # GAT
                if map_attention_heads is None:
                    layer_config[('map', 'connected_to', 'map')] = GATv2Conv((-1, -1), map_dims[conv_idx], add_self_loops=False)
                else:
                    layer_config[('map', 'connected_to', 'map')] = GATv2Conv((-1, -1), map_dims[conv_idx], add_self_loops=False, heads=map_attention_heads[conv_idx])
                
            conv = HeteroConv(layer_config, aggr='mean')
            self.convs.append(conv)

            
        # Temporal layers for player and map nodes
        self.player_temporal_layer = GConvGRU(in_channels=player_dims[-1], out_channels=player_dims[-1], K=2)
        self.map_temporal_layer = GConvGRU(in_channels=map_dims[-1], out_channels=map_dims[-1], K=2)



        # Create linear layer for the flattened input
        self.linear = Linear(-1, dense_layers[0]['input_neuron_num'])

        
        # Create dense layers based on the 'dense_layers' parameter
        dense_layers_container = []
        for layer_config in dense_layers:

            if layer_config['dropout'] == 0:
                # Add the first layer manually because it has a different input size
                dense_layers_container.append(torch.nn.Linear(layer_config['input_neuron_num'], layer_config['neuron_num']))
                
                # Add activation function if it is not None - the last layer does not have sigmoid activation function because of the BCEWithLogitsLoss
                if layer_config['activation_function'] is not None:
                    dense_layers_container.append(layer_config['activation_function'])

                # Add the rest of the layers (if there are any)
                for _ in range(layer_config['num_of_layers'] - 1):
                    dense_layers_container.append(torch.nn.Linear(layer_config['neuron_num'], layer_config['neuron_num']))

                    # Add activation function if it is not None - the last layer does not have sigmoid activation function because of the BCEWithLogitsLoss
                    if layer_config['activation_function'] is not None:
                        dense_layers_container.append(layer_config['activation_function'])
            else:
                dense_layers_container.append(torch.nn.Dropout(layer_config['dropout']))
        
        self.dense = torch.nn.Sequential(*dense_layers_container)
        





    # --------------------------------------------------
    # Forward pass
    # --------------------------------------------------

    def forward(self, x_dict, edge_index_dict, y, temporal_batch_size):

        
        # --------------------------------------------
        #             Spatial convolutions
        # --------------------------------------------

        conv_idx = 0
        for conv in self.convs:
            temp = conv(x_dict, edge_index_dict)
            
            if conv_idx < self.player_convs:
                x_dict['player'] = temp['player']

            if conv_idx < self.map_convs:
                x_dict['map'] = temp['map']
                
            x_dict = {key: torch.nn.functional.leaky_relu(x) for key, x in x_dict.items()}

            conv_idx += 1


        # --------------------------------------------
        #             Graph separation
        # --------------------------------------------

        # Container for the flattened graphs after the convolutions
        discrete_graphs = []

        # Collect the graphs from the batch
        for graph_idx in range(temporal_batch_size):

            # Get the actual graph
            actual_x_dict, actual_edge_index_dict = self.get_actual_graph(x_dict, edge_index_dict, graph_idx, temporal_batch_size)

            # Get the graph data
            graph_data = torch.tensor([
                y['round'][graph_idx],
                y['time'][graph_idx],
                y['remaining_time'][graph_idx],
                y['CT_alive_num'][graph_idx],
                y['T_alive_num'][graph_idx],
                y['CT_total_hp'][graph_idx],
                y['T_total_hp'][graph_idx],
                y['CT_equipment_value'][graph_idx],
                y['T_equipment_value'][graph_idx],
                y['CT_losing_streak'][graph_idx],
                y['T_losing_streak'][graph_idx],
                y['is_bomb_dropped'][graph_idx],
                y['is_bomb_being_planted'][graph_idx],
                y['is_bomb_being_defused'][graph_idx],
                y['is_bomb_planted_at_A_site'][graph_idx],
                y['is_bomb_planted_at_B_site'][graph_idx],
                y['bomb_X'][graph_idx],
                y['bomb_Y'][graph_idx],
                y['bomb_Z'][graph_idx],
                y['bomb_mx_pos1'][graph_idx],
                y['bomb_mx_pos2'][graph_idx],
                y['bomb_mx_pos3'][graph_idx],
                y['bomb_mx_pos4'][graph_idx],
                y['bomb_mx_pos5'][graph_idx],
                y['bomb_mx_pos6'][graph_idx],
                y['bomb_mx_pos7'][graph_idx],
                y['bomb_mx_pos8'][graph_idx],
                y['bomb_mx_pos9'][graph_idx],
            ]).to('cuda')

            discrete_graphs.append([actual_x_dict, actual_edge_index_dict, graph_data])

        # --------------------------------------------------
        #               Temporal convolutions
        # --------------------------------------------------

        # Flattened tensors
        flattened_graphs = []

        # Temporal layers
        for graph in discrete_graphs:

            temp_player = self.player_temporal_layer(graph[0]['player'], graph[1][('player', 'is', 'player')])
            temp_map = self.map_temporal_layer(graph[0]['map'], graph[1][('map', 'connected_to', 'map')])

            temp_player = F.leaky_relu(temp_player)
            temp_map = F.leaky_relu(temp_map)

            x = torch.cat([torch.flatten(temp_player), torch.flatten(temp_map), torch.flatten(graph[2])])
            flattened_graphs.append(x)

        # Stack the flattened graphs
        x = torch.stack(flattened_graphs).to('cuda')
        
        # --------------------------------------------------
        #                   Dense layers
        # --------------------------------------------------

        x = self.linear(x)
        x = F.leaky_relu(x)
        
        x = self.dense(x)
        
        return x
    






    # --------------------------------------------------
    # Helper functions
    # --------------------------------------------------

    def get_actual_graph(self, x_dict, edge_index_dict, graph_idx, batch_size):

        # Node feature dictionary for the actual graph
        actual_x_dict = {}

        SINGLE_PLAYER_NODE_SIZE = int(x_dict['player'].shape[0] / batch_size)
        SINGLE_MAP_NODE_SIZE = int(x_dict['map'].shape[0] / batch_size)

        actual_x_dict['player'] = x_dict['player'][graph_idx * SINGLE_PLAYER_NODE_SIZE: (graph_idx + 1) * SINGLE_PLAYER_NODE_SIZE, :]
        actual_x_dict['map'] = x_dict['map'][graph_idx * SINGLE_MAP_NODE_SIZE: (graph_idx + 1) * SINGLE_MAP_NODE_SIZE, :]


        

        # Edge index dictionary for the actual graph
        actual_edge_index_dict = {}

        SINGLE_PLAYER_TO_PLAYER_EDGE_SIZE = int(edge_index_dict[('player', 'is', 'player')].shape[1] / batch_size)
        SINGLE_PLAYER_TO_MAP_EDGE_SIZE = int(edge_index_dict[('player', 'closest_to', 'map')].shape[1] / batch_size)
        SINGLE_MAP_TO_MAP_EDGE_SIZE = int(edge_index_dict[('map', 'connected_to', 'map')].shape[1] / batch_size)

        actual_edge_index_dict[('map', 'connected_to', 'map')] = edge_index_dict[('map', 'connected_to', 'map')]             [:, graph_idx  *SINGLE_MAP_TO_MAP_EDGE_SIZE: (graph_idx + 1) * SINGLE_MAP_TO_MAP_EDGE_SIZE] - graph_idx * SINGLE_MAP_NODE_SIZE
        
        actual_edge_index_dict[('player', 'closest_to', 'map')] = edge_index_dict[('player', 'closest_to', 'map')]             [:, graph_idx * SINGLE_PLAYER_TO_MAP_EDGE_SIZE: (graph_idx + 1) * SINGLE_PLAYER_TO_MAP_EDGE_SIZE]
        
        player_to_map_correction_tensor = torch.tensor([SINGLE_PLAYER_NODE_SIZE * graph_idx, SINGLE_MAP_NODE_SIZE * graph_idx]).to('cuda')
        actual_edge_index_dict[('player', 'closest_to', 'map')] = actual_edge_index_dict[('player', 'closest_to', 'map')] - player_to_map_correction_tensor.view(-1, 1)
        
        actual_edge_index_dict[('player', 'is', 'player')] = edge_index_dict[('player', 'is', 'player')]             [:, graph_idx * SINGLE_PLAYER_TO_PLAYER_EDGE_SIZE: (graph_idx + 1) * SINGLE_PLAYER_TO_PLAYER_EDGE_SIZE]
        
        player_is_player_correction_tensor = torch.tensor([SINGLE_PLAYER_NODE_SIZE * graph_idx, SINGLE_PLAYER_NODE_SIZE * graph_idx]).to('cuda')
        actual_edge_index_dict[('player', 'is', 'player')] = actual_edge_index_dict[('player', 'is', 'player')] - player_is_player_correction_tensor.view(-1, 1)

        
        return actual_x_dict, actual_edge_index_dict
    
