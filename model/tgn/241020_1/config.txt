model: TemporalHeterogeneousGNN(
  (convs): ModuleList(
    (0-1): 2 x HeteroConv(num_relations=3)
    (2): HeteroConv(num_relations=1)
    (3): HeteroConv(num_relations=0)
  )
  (player_temporal_layer): GConvGRU(
    (conv_x_z): ChebConv(5, 5, K=1, normalization=sym)
    (conv_h_z): ChebConv(5, 5, K=1, normalization=sym)
    (conv_x_r): ChebConv(5, 5, K=1, normalization=sym)
    (conv_h_r): ChebConv(5, 5, K=1, normalization=sym)
    (conv_x_h): ChebConv(5, 5, K=1, normalization=sym)
    (conv_h_h): ChebConv(5, 5, K=1, normalization=sym)
  )
  (map_temporal_layer): GConvGRU(
    (conv_x_z): ChebConv(5, 5, K=2, normalization=sym)
    (conv_h_z): ChebConv(5, 5, K=2, normalization=sym)
    (conv_x_r): ChebConv(5, 5, K=2, normalization=sym)
    (conv_h_r): ChebConv(5, 5, K=2, normalization=sym)
    (conv_x_h): ChebConv(5, 5, K=2, normalization=sym)
    (conv_h_h): ChebConv(5, 5, K=2, normalization=sym)
  )
  (linear): Linear(1073, 64, bias=True)
  (dense): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.2, inplace=False)
    (3): Linear(in_features=32, out_features=16, bias=True)
    (4): LeakyReLU(negative_slope=0.01)
    (5): Linear(in_features=16, out_features=16, bias=True)
    (6): LeakyReLU(negative_slope=0.01)
    (7): Dropout(p=0.2, inplace=False)
    (8): Linear(in_features=16, out_features=4, bias=True)
    (9): LeakyReLU(negative_slope=0.01)
    (10): Linear(in_features=4, out_features=4, bias=True)
    (11): LeakyReLU(negative_slope=0.01)
    (12): Dropout(p=0.1, inplace=False)
    (13): Linear(in_features=4, out_features=1, bias=True)
  )
)
dense_layers: [{'dropout': 0, 'num_of_layers': 1, 'neuron_num': 32, 'input_neuron_num': 64, 'activation_function': LeakyReLU(negative_slope=0.01)}, {'dropout': 0.2}, {'dropout': 0, 'num_of_layers': 2, 'neuron_num': 16, 'input_neuron_num': 32, 'activation_function': LeakyReLU(negative_slope=0.01)}, {'dropout': 0.2}, {'dropout': 0, 'num_of_layers': 2, 'neuron_num': 4, 'input_neuron_num': 16, 'activation_function': LeakyReLU(negative_slope=0.01)}, {'dropout': 0.1}, {'dropout': 0, 'num_of_layers': 1, 'neuron_num': 1, 'input_neuron_num': 4, 'activation_function': None}]
player_dims: [20, 5]
map_dims: [20, 10, 5]
player_attention_heads: None
map_attention_heads: None
player_K: 2
map_K: 3
trainable_params: 83999
optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0006
    maximize: False
    weight_decay: 2e-07
)
loss_function: BCEWithLogitsLoss()
batch_size: 100
