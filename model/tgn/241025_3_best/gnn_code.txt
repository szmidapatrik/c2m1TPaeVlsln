
class TemporalHeterogeneousGNN(torch.nn.Module):


    def __init__(self, player_dims, map_dims, dense_layers, player_attention_heads=None, map_attention_heads=None, player_K=1, map_K=1):

        super().__init__()

        if player_attention_heads is not None and len(player_dims) != len(player_attention_heads):
            raise ValueError('The length of player dimensions and player attention heads arrays must be the same.')
        if map_attention_heads is not None and len(map_dims) != len(map_attention_heads):
            raise ValueError('The length of map dimensions and map attention heads arrays must be the same.')

        self.map_convs = len(map_dims)
        self.player_convs = len(player_dims)
        self.conv_layer_number = max([self.map_convs, self.player_convs])

        
        # Create convolutional layers
        self.convs = torch.nn.ModuleList()
        for conv_idx in range(self.conv_layer_number + 1):

            layer_config = {}

            if conv_idx < len(player_dims):
            
                if player_attention_heads is None:
                    layer_config[('player', 'is', 'player')] = GATv2Conv((-1, -1), player_dims[conv_idx], add_self_loops=False)
                else:
                    layer_config[('player', 'is', 'player')] = GATv2Conv((-1, -1), player_dims[conv_idx], add_self_loops=False, heads=player_attention_heads[conv_idx])

            if conv_idx < len(player_dims):
            
                if player_attention_heads is None:
                    layer_config[('player', 'closest_to', 'map')] = GATv2Conv((-1, -1), map_dims[conv_idx], add_self_loops=False)
                else:
                    layer_config[('player', 'closest_to', 'map')] = GATv2Conv((-1, -1), map_dims[conv_idx], add_self_loops=False, heads=map_attention_heads[conv_idx])

                
            if conv_idx < len(map_dims):

                if map_attention_heads is None:
                    layer_config[('map', 'connected_to', 'map')] = GATv2Conv((-1, -1), map_dims[conv_idx], add_self_loops=False)
                else:
                    layer_config[('map', 'connected_to', 'map')] = GATv2Conv((-1, -1), map_dims[conv_idx], add_self_loops=False, heads=map_attention_heads[conv_idx])
                
            conv = HeteroConv(layer_config, aggr='mean')
            self.convs.append(conv)

            
        # Temporal layers for player and map nodes
        self.player_temporal_layer = GConvGRU(in_channels=player_dims[-1], out_channels=player_dims[-1], K=player_K)
        self.map_temporal_layer = GConvGRU(in_channels=map_dims[-1], out_channels=map_dims[-1], K=map_K)



        # Create linear layer for the flattened input
        self.linear = Linear(-1, dense_layers[0]['input_neuron_num'])

        
        # Create dense layers based on the 'dense_layers' parameter
        dense_layers_container = []

        for layer_config in dense_layers:

            if layer_config['dropout'] == 0:
                # Add the first layer manually because it has a different input size
                dense_layers_container.append(torch.nn.Linear(layer_config['input_neuron_num'], layer_config['neuron_num']))
                
                # Add activation function if it is not None - the last layer does not have sigmoid activation function because of the BCEWithLogitsLoss
                if layer_config['activation_function'] is not None:
                    dense_layers_container.append(layer_config['activation_function'])

                # Add the rest of the layers (if there are any)
                for _ in range(layer_config['num_of_layers'] - 1):
                    dense_layers_container.append(torch.nn.Linear(layer_config['neuron_num'], layer_config['neuron_num']))

                    # Add activation function if it is not None - the last layer does not have sigmoid activation function because of the BCEWithLogitsLoss
                    if layer_config['activation_function'] is not None:
                        dense_layers_container.append(layer_config['activation_function'])
            else:
                dense_layers_container.append(torch.nn.Dropout(layer_config['dropout']))
        
        self.dense = torch.nn.Sequential(*dense_layers_container)
        






    
    

    def forward(self, batch, batch_size, temporal_batch_size):

        # --------------------------------------------
        #        Spatial convolution preparation
        # --------------------------------------------

        # Get the data from the batch and prepare for spatial convolutions
        spatial_conv_list = [hetero_graph for temporal_graph in batch for hetero_graph in temporal_graph]
        spatial_conv_loader = DataLoader(spatial_conv_list, batch_size=len(spatial_conv_list), shuffle=False)
        spatial_conv_data = next(iter(spatial_conv_loader)).to('cuda')

        # --------------------------------------------
        #             Spatial convolutions
        # --------------------------------------------

        conv_idx = 0
        for conv in self.convs:
        
            temp = conv(spatial_conv_data.x_dict, spatial_conv_data.edge_index_dict)

            if conv_idx < self.player_convs:
                spatial_conv_data['player'].x = F.leaky_relu(temp['player'])

            if conv_idx < self.map_convs:
                spatial_conv_data['map'].x = F.leaky_relu(temp['map'])

            conv_idx += 1

        
            

        # --------------------------------------------
        #       Temporal convolution preparation
        # --------------------------------------------

        # Container for the flattened graphs after the convolutions
        T_graphs = {}

        # Collect the graphs from the batch
        for graph_idx in range(batch_size * temporal_batch_size):

            # Get the actual graph
            actual_x_dict, actual_edge_index_dict = self.get_actual_graph(spatial_conv_data.x_dict, spatial_conv_data.edge_index_dict, graph_idx, batch_size * temporal_batch_size)

            # Get the graph data
            graph_data = {
                'round': spatial_conv_data.y['round'][graph_idx],
                'time': spatial_conv_data.y['time'][graph_idx],
                'remaining_time': spatial_conv_data.y['remaining_time'][graph_idx],
                'CT_alive_num': spatial_conv_data.y['CT_alive_num'][graph_idx],
                'T_alive_num': spatial_conv_data.y['T_alive_num'][graph_idx],
                'CT_total_hp': spatial_conv_data.y['CT_total_hp'][graph_idx],
                'T_total_hp': spatial_conv_data.y['T_total_hp'][graph_idx],
                'CT_equipment_value': spatial_conv_data.y['CT_equipment_value'][graph_idx],
                'T_equipment_value': spatial_conv_data.y['T_equipment_value'][graph_idx],
                'CT_losing_streak': spatial_conv_data.y['CT_losing_streak'][graph_idx],
                'T_losing_streak': spatial_conv_data.y['T_losing_streak'][graph_idx],
                'is_bomb_dropped': spatial_conv_data.y['is_bomb_dropped'][graph_idx],
                'is_bomb_being_planted': spatial_conv_data.y['is_bomb_being_planted'][graph_idx],
                'is_bomb_being_defused': spatial_conv_data.y['is_bomb_being_defused'][graph_idx],
                'is_bomb_planted_at_A_site': spatial_conv_data.y['is_bomb_planted_at_A_site'][graph_idx],
                'is_bomb_planted_at_B_site': spatial_conv_data.y['is_bomb_planted_at_B_site'][graph_idx],
                'bomb_X': spatial_conv_data.y['bomb_X'][graph_idx],
                'bomb_Y': spatial_conv_data.y['bomb_Y'][graph_idx],
                'bomb_Z': spatial_conv_data.y['bomb_Z'][graph_idx],
                'bomb_mx_pos1': spatial_conv_data.y['bomb_mx_pos1'][graph_idx],
                'bomb_mx_pos2': spatial_conv_data.y['bomb_mx_pos2'][graph_idx],
                'bomb_mx_pos3': spatial_conv_data.y['bomb_mx_pos3'][graph_idx],
                'bomb_mx_pos4': spatial_conv_data.y['bomb_mx_pos4'][graph_idx],
                'bomb_mx_pos5': spatial_conv_data.y['bomb_mx_pos5'][graph_idx],
                'bomb_mx_pos6': spatial_conv_data.y['bomb_mx_pos6'][graph_idx],
                'bomb_mx_pos7': spatial_conv_data.y['bomb_mx_pos7'][graph_idx],
                'bomb_mx_pos8': spatial_conv_data.y['bomb_mx_pos8'][graph_idx],
                'bomb_mx_pos9': spatial_conv_data.y['bomb_mx_pos9'][graph_idx],
            }

            # Get the time index and create a key for it in the T container
            time_index = (graph_idx % temporal_batch_size)
            if time_index not in T_graphs:
                T_graphs[time_index] = []

            # Reconstruct the graph data
            recunstructed_graph = HeteroData()

            # Reconstruct the node features (x)
            for node_type, x in actual_x_dict.items():
                recunstructed_graph[node_type].x = x

            # Reconstruct the edge indices (edge_index)
            for edge_type, edge_index in actual_edge_index_dict.items():
                recunstructed_graph[edge_type].edge_index = edge_index

            # Reconstruct the y values (y)
            for y_key, y_value in graph_data.items():
                recunstructed_graph['y'][y_key] = y_value

            T_graphs[time_index].append(recunstructed_graph)

        # --------------------------------------------------
        #               Temporal convolutions
        # --------------------------------------------------

        for time_idx in range(temporal_batch_size):

            # Get the data for the current time index
            time_idx_list = T_graphs[time_idx]
            time_idx_data_loader = DataLoader(time_idx_list, batch_size=len(time_idx_list), shuffle=False)
            time_idx_data = next(iter(time_idx_data_loader))

            # Temporal convolutions
            time_idx_data['player'].x = F.leaky_relu(self.player_temporal_layer(time_idx_data.x_dict['player'], time_idx_data.edge_index_dict[('player', 'is', 'player')]))
            time_idx_data['map'].x = F.leaky_relu(self.map_temporal_layer(time_idx_data.x_dict['map'], time_idx_data.edge_index_dict[('map', 'connected_to', 'map')]))

            # Update the T_graphs container
            T_graphs[time_idx] = time_idx_data

            


            

        # --------------------------------------------------
        #               Dense layer preparation
        # --------------------------------------------------

        # Reconstruct the original batch ordering
        flattened_graphs = []

        for graph_idx in range(batch_size):
            for time_idx in range(temporal_batch_size):

                actual_x_dict, actual_edge_index_dict = self.get_actual_graph(T_graphs[time_idx].x_dict, T_graphs[time_idx].edge_index_dict, graph_idx, batch_size)

                graph_data = torch.tensor([
                    T_graphs[time_idx]['y']['round'][graph_idx],
                    T_graphs[time_idx]['y']['time'][graph_idx],
                    T_graphs[time_idx]['y']['remaining_time'][graph_idx],
                    T_graphs[time_idx]['y']['CT_alive_num'][graph_idx],
                    T_graphs[time_idx]['y']['T_alive_num'][graph_idx],
                    T_graphs[time_idx]['y']['CT_total_hp'][graph_idx],
                    T_graphs[time_idx]['y']['T_total_hp'][graph_idx],
                    T_graphs[time_idx]['y']['CT_equipment_value'][graph_idx],
                    T_graphs[time_idx]['y']['T_equipment_value'][graph_idx],
                    T_graphs[time_idx]['y']['CT_losing_streak'][graph_idx],
                    T_graphs[time_idx]['y']['T_losing_streak'][graph_idx],
                    T_graphs[time_idx]['y']['is_bomb_dropped'][graph_idx],
                    T_graphs[time_idx]['y']['is_bomb_being_planted'][graph_idx],
                    T_graphs[time_idx]['y']['is_bomb_being_defused'][graph_idx],
                    T_graphs[time_idx]['y']['is_bomb_planted_at_A_site'][graph_idx],
                    T_graphs[time_idx]['y']['is_bomb_planted_at_B_site'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_X'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_Y'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_Z'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_mx_pos1'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_mx_pos2'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_mx_pos3'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_mx_pos4'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_mx_pos5'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_mx_pos6'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_mx_pos7'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_mx_pos8'][graph_idx],
                    T_graphs[time_idx]['y']['bomb_mx_pos9'][graph_idx],
                ]).to('cuda')
                
                x = torch.cat([torch.flatten(actual_x_dict['player']), torch.flatten(actual_x_dict['map']), torch.flatten(graph_data)])
                flattened_graphs.append(x)

                
        # Stack the flattened graphs
        x = torch.stack(flattened_graphs).to('cuda')
        
        # --------------------------------------------------
        #                   Dense layers
        # --------------------------------------------------

        x = self.linear(x)
        x = F.leaky_relu(x)
        x = self.dense(x)
        
        return x
    






    # --------------------------------------------------
    # Helper functions
    # --------------------------------------------------

    def get_actual_graph(self, x_dict, edge_index_dict, graph_idx, batch_size):

        # Node feature dictionary for the actual graph
        actual_x_dict = {}

        SINGLE_PLAYER_NODE_SIZE = int(x_dict['player'].shape[0] / batch_size)
        SINGLE_MAP_NODE_SIZE = int(x_dict['map'].shape[0] / batch_size)

        actual_x_dict['player'] = x_dict['player'][graph_idx * SINGLE_PLAYER_NODE_SIZE: (graph_idx + 1) * SINGLE_PLAYER_NODE_SIZE, :]
        actual_x_dict['map'] = x_dict['map'][graph_idx * SINGLE_MAP_NODE_SIZE: (graph_idx + 1) * SINGLE_MAP_NODE_SIZE, :]


        

        # Edge index dictionary for the actual graph
        actual_edge_index_dict = {}

        SINGLE_PLAYER_TO_PLAYER_EDGE_SIZE = int(edge_index_dict[('player', 'is', 'player')].shape[1] / batch_size)
        SINGLE_PLAYER_TO_MAP_EDGE_SIZE = int(edge_index_dict[('player', 'closest_to', 'map')].shape[1] / batch_size)
        SINGLE_MAP_TO_MAP_EDGE_SIZE = int(edge_index_dict[('map', 'connected_to', 'map')].shape[1] / batch_size)

        actual_edge_index_dict[('map', 'connected_to', 'map')] = edge_index_dict[('map', 'connected_to', 'map')]             [:, graph_idx  *SINGLE_MAP_TO_MAP_EDGE_SIZE: (graph_idx + 1) * SINGLE_MAP_TO_MAP_EDGE_SIZE] - graph_idx * SINGLE_MAP_NODE_SIZE
        
        actual_edge_index_dict[('player', 'closest_to', 'map')] = edge_index_dict[('player', 'closest_to', 'map')]             [:, graph_idx * SINGLE_PLAYER_TO_MAP_EDGE_SIZE: (graph_idx + 1) * SINGLE_PLAYER_TO_MAP_EDGE_SIZE]
        
        player_to_map_correction_tensor = torch.tensor([SINGLE_PLAYER_NODE_SIZE * graph_idx, SINGLE_MAP_NODE_SIZE * graph_idx]).to('cuda')
        actual_edge_index_dict[('player', 'closest_to', 'map')] = actual_edge_index_dict[('player', 'closest_to', 'map')] - player_to_map_correction_tensor.view(-1, 1)
        
        actual_edge_index_dict[('player', 'is', 'player')] = edge_index_dict[('player', 'is', 'player')]             [:, graph_idx * SINGLE_PLAYER_TO_PLAYER_EDGE_SIZE: (graph_idx + 1) * SINGLE_PLAYER_TO_PLAYER_EDGE_SIZE]
        
        player_is_player_correction_tensor = torch.tensor([SINGLE_PLAYER_NODE_SIZE * graph_idx, SINGLE_PLAYER_NODE_SIZE * graph_idx]).to('cuda')
        actual_edge_index_dict[('player', 'is', 'player')] = actual_edge_index_dict[('player', 'is', 'player')] - player_is_player_correction_tensor.view(-1, 1)

        
        return actual_x_dict, actual_edge_index_dict
    
