{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Torch\n","import torch\n","from torch_geometric.data import DataLoader\n","from torch_geometric.loader import DataLoader\n","\n","from torch_geometric.nn import SAGEConv, DenseSAGEConv, GATConv, HeteroConv, Linear\n","from torch_geometric.nn import aggr\n","\n","# Data manipulation\n","import pandas as pd\n","import numpy as np\n","\n","# Data visualization\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","# Scikit-learn\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc, brier_score_loss\n","\n","# Misc\n","import os\n","import time\n","from math import ceil\n","import random\n","from IPython.display import clear_output\n","from termcolor import colored\n","\n","\n","# Torch and CUDA options\n","torch.manual_seed(42)\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","# Path constants\n","PATH_GRAPH_DATA = '../../data/matches-processed/cs2/hetero-graph/'\n","PATH_MODELS = '../../model/gnn/'"]},{"cell_type":"markdown","metadata":{},"source":["## Data Loaders"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["BATCH_SIZE = 500\n","\n","\n","def print_ct_win_percentage(dataset):\n","    ct_wins = 0\n","    for data in dataset:\n","        ct_wins += data.y['CT_wins']\n","\n","    print('CT wins %:', ct_wins / len(dataset))"]},{"cell_type":"markdown","metadata":{},"source":["### Training data"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train data length: 714000\n","CT wins %: 0.46100700280112045\n"]}],"source":["train_data = torch.load(PATH_GRAPH_DATA + 'train.pt', weights_only=False)\n","\n","random.shuffle(train_data)\n","if len(train_data) % BATCH_SIZE != 0:\n","    train_data = train_data[:-(len(train_data) % BATCH_SIZE)]\n","\n","print('Train data length:', len(train_data))\n","print_ct_win_percentage(train_data)\n","\n","train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Validation data"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation data length: 224000\n","CT wins %: 0.4752857142857143\n"]}],"source":["val_data = torch.load(PATH_GRAPH_DATA + 'val.pt', weights_only=False)\n","\n","random.shuffle(val_data)\n","if len(val_data) % BATCH_SIZE != 0:\n","    val_data = val_data[:-(len(val_data) % BATCH_SIZE)]\n","\n","print('Validation data length:', len(val_data))\n","print_ct_win_percentage(val_data)\n","\n","val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["----\n","## Heterogeneous GNN"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class HeterogeneousGNN(torch.nn.Module):\n","\n","    # --------------------------------------------------\n","    # Initialization\n","    # --------------------------------------------------\n","\n","    def __init__(self, player_dims, map_dims, dense_layers):\n","\n","        super().__init__()\n","\n","        self.conv_layer_number = max([len(player_dims), len(map_dims)])\n","        self.player_convs = len(player_dims)\n","        self.map_convs = len(map_dims)\n","\n","        # Create convolutional layers\n","        self.convs = torch.nn.ModuleList()\n","        for conv_idx in range(self.conv_layer_number):\n","\n","            layer_config = {}\n","\n","            if conv_idx < len(player_dims):\n","                layer_config[('player', 'is', 'player')] = GATConv((-1, -1), player_dims[conv_idx], add_self_loops=False)\n","\n","            if conv_idx < len(player_dims):\n","                layer_config[('player', 'closest_to', 'map')] = GATConv((-1, -1), map_dims[conv_idx], add_self_loops=False)\n","\n","            if conv_idx < len(map_dims):\n","                layer_config[('map', 'connected_to', 'map')] = GATConv((-1, -1), map_dims[conv_idx], add_self_loops=False)\n","\n","            conv = HeteroConv(layer_config, aggr='mean')\n","            self.convs.append(conv)\n","\n","\n","\n","        # Create linear layer for the flattened input\n","        self.linear = Linear(-1, dense_layers[0]['input_neuron_num'])\n","\n","        \n","        # Create dense layers based on the 'dense_layers' parameter\n","        dense_layers_container = []\n","        for layer_config in dense_layers:\n","\n","            if layer_config['dropout'] == 0:\n","                # Add the first layer manually because it has a different input size\n","                dense_layers_container.append(torch.nn.Linear(layer_config['input_neuron_num'], layer_config['neuron_num']))\n","                \n","                # Add activation function if it is not None - the last layer does not have sigmoid activation function because of the BCEWithLogitsLoss\n","                if layer_config['activation_function'] is not None:\n","                    dense_layers_container.append(layer_config['activation_function'])\n","\n","                # Add the rest of the layers (if there are any)\n","                for _ in range(layer_config['num_of_layers'] - 1):\n","                    dense_layers_container.append(torch.nn.Linear(layer_config['neuron_num'], layer_config['neuron_num']))\n","\n","                    # Add activation function if it is not None - the last layer does not have sigmoid activation function because of the BCEWithLogitsLoss\n","                    if layer_config['activation_function'] is not None:\n","                        dense_layers_container.append(layer_config['activation_function'])\n","            else:\n","                dense_layers_container.append(torch.nn.Dropout(layer_config['dropout']))\n","        \n","        self.dense = torch.nn.Sequential(*dense_layers_container)\n","        \n","\n","\n","\n","\n","\n","    # --------------------------------------------------\n","    # Forward pass\n","    # --------------------------------------------------\n","\n","    def forward(self, x_dict, edge_index_dict, y, batch_size):\n","\n","        # Do the convolutions\n","        conv_idx = 1\n","        for conv in self.convs:\n","            temp = conv(x_dict, edge_index_dict)\n","            \n","            if conv_idx < self.player_convs:\n","                x_dict['player'] = temp['player']\n","\n","            if conv_idx < self.map_convs:\n","                x_dict['map'] = temp['map']\n","                \n","            x_dict = {key: torch.nn.functional.leaky_relu(x) for key, x in x_dict.items()}\n","\n","            conv_idx += 1\n","\n","\n","        # Container for the flattened graphs after the convolutions\n","        flattened_graphs = []\n","\n","        # Do the convolutions for each graph in the batch\n","        for graph_idx in range(batch_size):\n","\n","            # Get the actual graph\n","            actual_x_dict, actual_edge_index_dict = self.get_actual_graph(x_dict, edge_index_dict, graph_idx, batch_size)\n","\n","            # Get the graph data\n","            graph_data = torch.tensor([\n","                y['round'][graph_idx],\n","                y['time'][graph_idx],\n","                y['remaining_time'][graph_idx],\n","                y['CT_alive_num'][graph_idx],\n","                y['T_alive_num'][graph_idx],\n","                y['CT_total_hp'][graph_idx],\n","                y['T_total_hp'][graph_idx],\n","                y['CT_equipment_value'][graph_idx],\n","                y['T_equipment_value'][graph_idx],\n","                y['CT_losing_streak'][graph_idx],\n","                y['T_losing_streak'][graph_idx],\n","                y['is_bomb_dropped'][graph_idx],\n","                y['is_bomb_being_planted'][graph_idx],\n","                y['is_bomb_being_defused'][graph_idx],\n","                y['is_bomb_planted_at_A_site'][graph_idx],\n","                y['is_bomb_planted_at_B_site'][graph_idx],\n","                y['bomb_X'][graph_idx],\n","                y['bomb_Y'][graph_idx],\n","                y['bomb_Z'][graph_idx],\n","                y['bomb_mx_pos1'][graph_idx],\n","                y['bomb_mx_pos2'][graph_idx],\n","                y['bomb_mx_pos3'][graph_idx],\n","                y['bomb_mx_pos4'][graph_idx],\n","                y['bomb_mx_pos5'][graph_idx],\n","                y['bomb_mx_pos6'][graph_idx],\n","                y['bomb_mx_pos7'][graph_idx],\n","                y['bomb_mx_pos8'][graph_idx],\n","                y['bomb_mx_pos9'][graph_idx],\n","            ]).to('cuda')\n","\n","            # Create the flattened input tensor and append it to the container\n","            x = torch.cat([torch.flatten(actual_x_dict['player']), torch.flatten(actual_x_dict['map']), torch.flatten(graph_data)])\n","\n","            flattened_graphs.append(x)\n","\n","        # Stack the flattened graphs\n","        x = torch.stack(flattened_graphs).to('cuda')\n","\n","        x = self.linear(x)\n","        x = torch.nn.functional.leaky_relu(x)\n","        \n","        x = self.dense(x)\n","        \n","        return x\n","    \n","\n","\n","\n","\n","\n","\n","    # --------------------------------------------------\n","    # Helper functions\n","    # --------------------------------------------------\n","\n","    def get_actual_graph(self, x_dict, edge_index_dict, graph_idx, batch_size):\n","\n","        # Node feature dictionary for the actual graph\n","        actual_x_dict = {}\n","\n","        single_player_node_size = int(x_dict['player'].shape[0] / batch_size)\n","        single_map_node_size = int(x_dict['map'].shape[0] / batch_size)\n","\n","        actual_x_dict['player'] = x_dict['player'][graph_idx*single_player_node_size:(graph_idx+1)*single_player_node_size, :]\n","        actual_x_dict['map'] = x_dict['map'][graph_idx*single_map_node_size:(graph_idx+1)*single_map_node_size, :]\n","\n","\n","        # Edge index dictionary for the actual graph\n","        actual_edge_index_dict = {}\n","\n","        single_map_to_map_edge_size = int(edge_index_dict[('map', 'connected_to', 'map')].shape[1] / batch_size)\n","        single_player_to_map_edge_size = int(edge_index_dict[('player', 'closest_to', 'map')].shape[1] / batch_size)\n","\n","        actual_edge_index_dict[('map', 'connected_to', 'map')] = edge_index_dict[('map', 'connected_to', 'map')] \\\n","            [:, graph_idx*single_map_to_map_edge_size:(graph_idx+1)*single_map_to_map_edge_size] \\\n","            - graph_idx*single_map_node_size\n","        \n","        actual_edge_index_dict[('player', 'closest_to', 'map')] = edge_index_dict[('player', 'closest_to', 'map')] \\\n","            [:, graph_idx*single_player_to_map_edge_size:(graph_idx+1)*single_player_to_map_edge_size]\n","        \n","        actual_edge_index_dict_correction_tensor = torch.tensor([single_player_node_size*graph_idx, single_map_node_size*graph_idx]).to('cuda')\n","        actual_edge_index_dict[('player', 'closest_to', 'map')] = actual_edge_index_dict[('player', 'closest_to', 'map')] - actual_edge_index_dict_correction_tensor.view(-1, 1)\n","\n","        \n","        return actual_x_dict, actual_edge_index_dict\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["### Training functions"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def train(model, optimizer, loss_function, train_loader, val_loaders, batch_size, epochs, trainable_params, save_path):\n","\n","    torch.cuda.empty_cache()\n","\n","    train_losses = []\n","    val_losses = []\n","    accuracies = []\n","    brier_scores = []\n","    precisions = []\n","    recalls = []\n","    f1_scores = []\n","\n","    for epoch in range(1, epochs + 1):\n","\n","        epoch_start = time.time()\n","        model.train()\n","\n","        total_loss = 0\n","\n","        for data in train_loader:  # Iterate in batches over the training dataset.\n","\n","            data = data.to('cuda')\n","            optimizer.zero_grad()  # Clear gradients.\n","            \n","            out = model(data.x_dict, data.edge_index_dict, data.y, batch_size).float()  # Perform a single forward pass.\n","            target = torch.tensor(data.y['CT_wins']).float().to('cuda') \n","\n","            loss = loss_function(out.squeeze(), target)  # Compute the loss.\n","            loss.backward()  # Backpropagate.\n","            optimizer.step()  # Update model parameters.\n","\n","            total_loss += loss.item()  # Accumulate the loss.\n","\n","\n","        # Calculate train and validation metrics\n","        train_avg_loss = total_loss / len(train_loader)\n","        val_avg_loss, val_accuracy, val_brier_score, val_precision, val_recall, val_f1, val_conf_mx, val_fpr, val_tpr, val_sorted_predictions = validation(model, loss_function, val_loaders, batch_size) \n","\n","        # Epoch information\n","        epoch_end = time.time()\n","        epoch_duration = epoch_end - epoch_start\n","        epoch_end_time = time.asctime(time.localtime()) \n","\n","        # Save metrics\n","        train_losses.append(train_avg_loss)\n","        val_losses.append(val_avg_loss)\n","        accuracies.append(val_accuracy)\n","        brier_scores.append(val_brier_score)\n","        precisions.append(val_precision)\n","        recalls.append(val_recall)\n","        f1_scores.append(val_f1)\n","\n","\n","        # Save model\n","        torch.save(model, PATH_MODELS + save_path + f'epoch_{epoch}.pt')\n","\n","        # Visualize epoch\n","        epoch_result_visualization(\n","            epochs, \n","            epoch,\n","            epoch_end_time,\n","            epoch_duration,\n","\n","            train_losses, \n","            val_losses, \n","            accuracies,\n","            brier_scores,\n","            precisions, \n","            recalls, \n","            f1_scores,\n","            val_conf_mx,\n","            val_fpr, val_tpr,\n","            val_sorted_predictions,\n","\n","            trainable_params,\n","            save_path\n","        )\n","\n","        \n","\n","def validation(model, loss_function, val_loader, batch_size):\n","    \n","    # Switch to evaluation mode.\n","    model.eval()\n","\n","    total_loss = 0\n","\n","    predictions_all = []\n","    targets_all = []\n","\n","    with torch.no_grad():\n","        \n","        for data in val_loader:  # Iterate in batches over the validation dataset.\n","            data = data.to('cuda')\n","\n","            out = model(data.x_dict, data.edge_index_dict, data.y, batch_size).float()\n","            target = torch.tensor(data.y['CT_wins']).float().to('cuda')\n","\n","            loss = loss_function(out.squeeze(), target)\n","            total_loss += loss.item()\n","\n","            # Get the predictions\n","            predictions = torch.sigmoid(out).float()\n","\n","            predictions_all.extend(predictions.cpu().numpy())\n","            targets_all.extend(target.cpu().numpy())\n","\n","    # Prediction formattings\n","    predictions_all = np.array(predictions_all).flatten().tolist()\n","    sorted_predictions = np.sort(predictions_all).tolist()\n","    rounded_predictions = np.round(predictions_all)\n","\n","    # Calculate metrics\n","    val_avg_loss = total_loss / len(val_loader)\n","    accuracy = accuracy_score(targets_all, rounded_predictions)\n","    brier_score = brier_score_loss(targets_all, predictions_all)\n","    precision = precision_score(targets_all, rounded_predictions)\n","    recall = recall_score(targets_all, rounded_predictions)\n","    f1 = f1_score(targets_all, rounded_predictions)\n","    cm = confusion_matrix(targets_all, rounded_predictions)\n","    fpr, tpr, _ = roc_curve(targets_all, predictions_all)\n","\n","    return val_avg_loss, accuracy, brier_score, precision, recall, f1, cm, fpr, tpr, sorted_predictions\n","\n","\n","\n","def epoch_result_visualization(\n","    epochs,\n","    epoch,\n","    epoch_end_time,\n","    epoch_duration,\n","\n","    train_losses,\n","    val_losses,\n","    accuracies, \n","    brier_scores,\n","    precisions, \n","    recalls, \n","    f1_scores,\n","    conf_mx,\n","    fpr, tpr,\n","    sorted_predictions,\n","\n","    trainable_params,\n","    save_path\n","):\n","    # Clear the output\n","    clear_output(wait=True)\n","\n","    # Print the results\n","    print_epoch_results(epoch, epoch_end_time, epoch_duration, train_losses[-1], val_losses[-1], accuracies[-1], brier_scores[-1], precisions[-1], recalls[-1], f1_scores[-1], trainable_params, save_path)\n","    \n","    \n","    # Visualize the confusion matrix, ROC/AUC curve and model calibration plot\n","    visualize_CM_ROCAUC_CAL(conf_mx, fpr, tpr, sorted_predictions, save_path, epoch)\n","\n","\n","    # Visualize the training and validation loss\n","    visualize_train_val_metrics(epochs, epoch, train_losses, val_losses, accuracies, brier_scores, precisions, recalls, f1_scores, save_path)\n","\n","def print_epoch_results(epoch, epoch_end_time, epoch_duration, train_avg_loss, val_avg_loss, accuracy, brier_score, precision, recall, f1, trainable_params, save_path):\n","\n","    print('Trainable parameters: ' + str(trainable_params))\n","    print(\"-------------------------------------------------------------------------------------------\\n\"\n","         f\"                                     Epoch {epoch}\\n\"\n","          \"-------------------------------------------------------------------------------------------\\n\" + \n","          colored('Time: \\n', \"light_blue\", attrs=[\"bold\"]) +\n","         f\"   -End time: {epoch_end_time} \\n\"\n","         f\"   -Duration: {epoch_duration}sec ({epoch_duration / 60} min) \\n\"\n","          \". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\" +\n","          colored('Training results: \\n', \"light_blue\", attrs=[\"bold\"]) +\n","         f\"   - Average loss: {train_avg_loss:.4f} \\n\"\n","          \". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\" +\n","          colored('Validation results: \\n', \"light_blue\", attrs=[\"bold\"]) +\n","         f\"   - Average loss: {val_avg_loss} \\n\"\n","         f\"   - Accuracy: {accuracy} \\n\"\n","         f\"   - Brier score: {brier_score} \\n\"\n","         f\"   - Precision: {precision} \\n\"\n","         f\"   - Recall: {recall} \\n\"\n","         f\"   - F1: {f1} \\n\"\n","          \". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\")\n","    \n","    with open(PATH_MODELS + save_path + f'epoch_{epoch}_metrics.txt', 'w') as file:\n","        file.write(\n","          \"Trainable parameters: \" + str(trainable_params) + \"\\n\"\n","          \"-------------------------------------------------------------------------------------------\\n\"\n","         f\"                                     Epoch {epoch}\\n\"\n","          \"-------------------------------------------------------------------------------------------\\n\"\n","          \"Time: \\n\"\n","         f\"   -End time: {epoch_end_time}s \\n\"\n","         f\"   -Duration: {epoch_duration}s \\n\"\n","          \". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\"\n","          \"Training results:\\n\"\n","         f\"   - Average loss: {train_avg_loss:.4f} \\n\"\n","          \". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\"\n","          \"Validation results: \\n\"\n","         f\"   - Average loss: {val_avg_loss} \\n\"\n","         f\"   - Accuracy: {accuracy} \\n\"\n","         f\"   - Brier score: {brier_score} \\n\"\n","         f\"   - Precision: {precision} \\n\"\n","         f\"   - Recall: {recall} \\n\"\n","         f\"   - F1: {f1} \\n\"\n","          \". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\")\n","\n","def visualize_CM_ROCAUC_CAL(cm, fpr, tpr, sorted_predictions, save_path, epoch):\n","\n","    fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n","    fig.suptitle(f'Epoch {epoch} - Validation scores', fontsize=12)\n","\n","\n","    # Confusion Matrix\n","    im = axs[0].matshow(cm, cmap='Blues', alpha=0.6)\n","    axs[0].set_aspect('equal')\n","    axs[0].set_title('Confusion matrix', fontsize=10, y=1)\n","    axs[0].set_xlabel('Predicted', fontsize=8)\n","    axs[0].set_ylabel('Actual', fontsize=8)\n","    # Add numbers to the confusion matrix\n","    for (i, j), val in np.ndenumerate(cm):\n","        axs[0].text(j, i, f'{val}', ha='center', va='center', fontsize=10, color='black')\n","    # Set x-ticks to bottom\n","    axs[0].set_xticks([0, 1])\n","    axs[0].set_xticklabels([0, 1])\n","    axs[0].tick_params(axis='x', bottom=True, top=False, labelbottom=True, labeltop=False)\n","\n","\n","    # ROC/AUC Curve\n","    axs[1].plot(fpr, tpr, color='darkorange', lw=2, label='AUC = %0.2f' % auc(fpr, tpr))\n","    axs[1].plot([0, 1], [0, 1], color='black', lw=1, linestyle='--')\n","    axs[1].set_xlim([0.0, 1.0])\n","    axs[1].set_ylim([0.0, 1.05])\n","    axs[1].set_xlabel('False Positive Rate', fontsize=8)\n","    axs[1].set_ylabel('True Positive Rate', fontsize=8)\n","    axs[1].set_title('ROC/AUC Curve', fontsize=10, y=1)\n","    axs[1].legend(loc=\"lower right\", fontsize=8)\n","    axs[1].set_aspect('equal')\n","\n","    # Model calibration plot\n","    axs[2].plot(np.linspace(1, len(sorted_predictions), len(sorted_predictions)), sorted_predictions, lw=2)\n","    axs[2].set_ylim([0.0, 1.0])\n","    axs[2].plot([0, len(sorted_predictions)], [0, 1], color='black', lw=1, linestyle='--')\n","    axs[2].set_xlabel('Samples', fontsize=8)\n","    axs[2].set_ylabel('Predicted Proba', fontsize=8)\n","    axs[2].set_title('Calibration curve', fontsize=10, y=1)\n","    axs[2].set_aspect('auto')\n","\n","    plt.tight_layout()\n","    plt.savefig(PATH_MODELS + save_path + f'epoch_{epoch}_val_conf_AUC_cal.png')\n","    plt.show()\n","\n","def visualize_train_val_metrics(epochs, epoch, train_losses, val_losses, accuracies, brier_scores, precisions, recalls, f1_scores, save_path):\n","\n","    # ----------------------------------------\n","    # Metrics visualization\n","    # ----------------------------------------\n","\n","    fig, axs = plt.subplots(2, 4, figsize=(11, 4.5))\n","    fig.suptitle('Training and Validation Metrics Over Epochs', fontsize=12)\n","\n","    epochs_range = np.arange(1, epoch + 1)\n","\n","    def plot_metric(ax, data, title, ylabel, color):\n","        if len(data) == 1:\n","            ax.scatter(epochs_range, data, s=10, c=color)\n","        else:\n","            ax.plot(epochs_range, data, marker='o', linestyle='-', color=color)\n","        ax.set_title(title, fontsize=10)\n","        ax.set_xlabel('Epoch', fontsize=8)\n","        ax.set_ylabel(ylabel, fontsize=8)\n","        ax.set_xticks(epochs_range)\n","        ax.set_xlim(-1, epochs)\n","        if 'Loss' not in title:\n","            ax.set_ylim(0, 1)\n","        ax.grid(True)\n","\n","    plot_metric(axs[0, 0], train_losses, 'Train Loss', 'Loss', 'cornflowerblue')\n","    plot_metric(axs[0, 1], val_losses, 'Validation Loss', 'Loss', 'darkorange')\n","    plot_metric(axs[0, 2], accuracies, 'Accuracy', 'Accuracy', 'limegreen')\n","    plot_metric(axs[0, 3], brier_scores, 'Brier score', 'Brier score', 'slateblue')\n","    plot_metric(axs[1, 0], precisions, 'Precision', 'Precision', 'red')\n","    plot_metric(axs[1, 1], recalls, 'Recall', 'Recall', 'teal')\n","    plot_metric(axs[1, 2], f1_scores, 'F1 Score', 'F1 Score', 'black')\n","\n","    plt.tight_layout()\n","    plt.savefig(PATH_MODELS + save_path + f'epoch_{epoch}_val_metrics.png')\n","    plt.show()\n","\n","\n","\n","# TODO: rewrite according to updated train and validation functions\n","def test(model, loss_function, test_loader, batch_size):\n","    model.eval()  # Switch to evaluation mode.\n","    total_loss = 0\n","    total_samples = 0\n","    predictions_all = []\n","    targets_all = []\n","\n","    with torch.no_grad():\n","        for data in test_loader:  # Iterate in batches over the test dataset.\n","            data = data.to('cuda')\n","\n","            out = model(data.x_dict, data.edge_index_dict, data.y, batch_size).float()\n","            target = torch.tensor(data.y['CT_wins']).float().to('cuda')\n","\n","            loss = loss_function(out.squeeze(), target)\n","            total_loss += loss.item()\n","            total_samples += len(target)\n","\n","            # Get the predictions\n","            predictions = torch.sigmoid(out).float()\n","\n","            predictions_all.extend(predictions.cpu().numpy())\n","            targets_all.extend(target.cpu().numpy())\n","\n","    # Calculate performance metrics\n","    rounded_predictions = np.round(predictions_all)\n","    \n","    sorted_predictions = [pred[0] for pred in predictions_all]\n","    sorted_predictions = np.sort(sorted_predictions)\n","\n","    avg_loss = total_loss / len(test_loader)\n","    accuracy = accuracy_score(targets_all, rounded_predictions)\n","    precision = precision_score(targets_all, rounded_predictions)\n","    recall = recall_score(targets_all, rounded_predictions)\n","    f1 = f1_score(targets_all, rounded_predictions)\n","    cm = confusion_matrix(targets_all, rounded_predictions)\n","    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","    fpr, tpr, _ = roc_curve(targets_all, predictions_all)\n","\n","    print(\"-------------------------------------------------------------------------------------------\\n\"\n","          \"                                      Test Results\\n\"                                     \n","          \"-------------------------------------------------------------------------------------------\\n\"\n","         f\"Average loss: {avg_loss} \\n\"\n","         f\"Accuracy: {accuracy} \\n\"\n","         f\"Precision: {precision} \\n\"\n","         f\"Recall: {recall} \\n\"\n","         f\"F1: {f1} \\n\"\n","          \"Confusion matrix & ROC/AUC\\n\"\n","          \". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\")\n","\n","    # ----------------------------------------\n","    # Confusion matrix & ROC/AUC visualization\n","    # ----------------------------------------\n","\n","    fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n","    fig.suptitle('Test results', fontsize=12)\n","\n","\n","    # Confusion Matrix\n","    im = axs[0].matshow(cm, cmap='Blues', alpha=0.6)\n","    axs[0].set_aspect('equal')\n","    axs[0].set_title('Confusion matrix', fontsize=10, y=1)\n","    axs[0].set_xlabel('Predicted', fontsize=8)\n","    axs[0].set_ylabel('Actual', fontsize=8)\n","    # Add numbers to the confusion matrix\n","    for (i, j), val in np.ndenumerate(cm):\n","        axs[0].text(j, i, f'{val}', ha='center', va='center', fontsize=10, color='black')\n","    # Set x-ticks to bottom\n","    axs[0].set_xticks([0, 1])\n","    axs[0].set_xticklabels([0, 1])\n","    axs[0].tick_params(axis='x', bottom=True, top=False, labelbottom=True, labeltop=False)\n","\n","\n","    # ROC/AUC Curve\n","    axs[1].plot(fpr, tpr, color='darkorange', lw=2, label='AUC = %0.2f' % auc(fpr, tpr))\n","    axs[1].plot([0, 1], [0, 1], color='black', lw=1, linestyle='--')\n","    axs[1].set_xlim([0.0, 1.0])\n","    axs[1].set_ylim([0.0, 1.05])\n","    axs[1].set_xlabel('False Positive Rate', fontsize=8)\n","    axs[1].set_ylabel('True Positive Rate', fontsize=8)\n","    axs[1].set_title('ROC/AUC Curve', fontsize=10, y=1)\n","    axs[1].legend(loc=\"lower right\", fontsize=8)\n","    axs[1].set_aspect('equal')\n","\n","    # Model calibration plot\n","    axs[2].plot(np.linspace(1, len(sorted_predictions), len(sorted_predictions)), sorted_predictions, lw=2)\n","    axs[2].set_ylim([0.0, 1.0])\n","    axs[2].plot([0, len(sorted_predictions)], [0, 1], color='black', lw=1, linestyle='--')\n","    axs[2].set_xlabel('Samples', fontsize=8)\n","    axs[2].set_ylabel('Predicted Proba', fontsize=8)\n","    axs[2].set_title('Calibration curve', fontsize=10, y=1)\n","    axs[2].set_aspect('auto')\n","\n","\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dense_layers = [\n","    {\n","        \"dropout\": 0,\n","        \"num_of_layers\": 1,\n","        \"neuron_num\": 32,\n","        \"input_neuron_num\": 64,\n","        \"activation_function\": torch.nn.LeakyReLU()\n","    },\n","    {\n","        \"dropout\": 0.5,\n","    },\n","    {\n","        \"dropout\": 0,\n","        \"num_of_layers\": 1,\n","        \"neuron_num\": 4,\n","        \"input_neuron_num\": 32,\n","        \"activation_function\": torch.nn.LeakyReLU()\n","    },\n","    {\n","        \"dropout\": 0.2,\n","    },\n","    {\n","        \"dropout\": 0,\n","        \"num_of_layers\": 1,\n","        \"neuron_num\": 1,\n","        \"input_neuron_num\": 4,\n","        \"activation_function\": None\n","    },\n","]\n","save_path = '240929_11/'\n","\n","model = HeterogeneousGNN(player_dims=[20, 5], map_dims=[15, 10, 5, 5, 3], dense_layers=dense_layers).to('cuda')\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-2, weight_decay=0.05)\n","loss_function = torch.nn.BCEWithLogitsLoss()\n","\n","# Initialization\n","with torch.no_grad():\n","    for batch in train_loader:\n","        batch = batch.to('cuda')\n","        out = model(batch.x_dict, batch.edge_index_dict, batch.y, BATCH_SIZE)\n","        break\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","train(model, optimizer, loss_function, train_loader, val_loader, BATCH_SIZE, 20, trainable_params, save_path)"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}
